# Evidence Pack System

## Overview
The Evidence Pack System provides complete audit trails and proof for every signal generated by RIVL. Each pack contains all source documents, calculations, and attribution data that led to a specific investment signal, enabling full transparency and trust in the system's recommendations.

## JSON Schema Definition

### Core Evidence Pack Structure
```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "RIVL Evidence Pack",
  "type": "object",
  "required": ["as_of", "ticker", "surprise_index", "regime_probs", "signals", "decision"],
  "properties": {
    "as_of": {
      "type": "string",
      "format": "date-time",
      "description": "Timestamp when evidence pack was generated"
    },
    "ticker": {
      "type": "string",
      "pattern": "^[A-Z]{1,5}$",
      "description": "Stock ticker symbol"
    },
    "surprise_index": {
      "type": "number",
      "minimum": 0,
      "maximum": 1,
      "description": "Surprise Index value (0-1)"
    },
    "regime_probs": {
      "type": "object",
      "properties": {
        "risk_on": {"type": "number", "minimum": 0, "maximum": 1},
        "risk_off": {"type": "number", "minimum": 0, "maximum": 1},
        "transition": {"type": "number", "minimum": 0, "maximum": 1},
        "goldilocks": {"type": "number", "minimum": 0, "maximum": 1}
      }
    },
    "signals": {
      "type": "array",
      "items": {
        "$ref": "#/definitions/signal"
      }
    },
    "consensus_gap": {
      "type": "number",
      "minimum": -1,
      "maximum": 1
    },
    "decision": {
      "$ref": "#/definitions/decision"
    },
    "metadata": {
      "$ref": "#/definitions/metadata"
    }
  }
}
```

### Signal Definition
```json
{
  "definitions": {
    "signal": {
      "type": "object",
      "required": ["type", "value", "confidence", "evidence"],
      "properties": {
        "type": {
          "type": "string",
          "enum": ["DeltaFilings", "NoveltyVelocity", "TruthMeter", "ConsensusGap", "Macro", "Policy"]
        },
        "value": {
          "type": "number",
          "minimum": -1,
          "maximum": 1
        },
        "confidence": {
          "type": "number",
          "minimum": 0,
          "maximum": 1
        },
        "evidence": {
          "type": "array",
          "items": {
            "$ref": "#/definitions/evidence_item"
          }
        }
      }
    }
  }
}
```

### Evidence Item Definition
```json
{
  "definitions": {
    "evidence_item": {
      "type": "object",
      "required": ["source_type", "source_url", "timestamp", "relevance_score"],
      "properties": {
        "source_type": {
          "type": "string",
          "enum": ["filing", "news", "transcript", "market_data", "macro_data"]
        },
        "source_url": {
          "type": "string",
          "format": "uri"
        },
        "timestamp": {
          "type": "string",
          "format": "date-time"
        },
        "relevance_score": {
          "type": "number",
          "minimum": 0,
          "maximum": 1
        },
        "snippet": {
          "type": "string",
          "maxLength": 500
        },
        "section": {
          "type": "string",
          "description": "Document section (e.g., 'Risk Factors', 'MD&A')"
        },
        "metadata": {
          "type": "object",
          "additionalProperties": true
        }
      }
    }
  }
}
```

## Implementation

### Evidence Pack Generator
```python
class EvidencePackGenerator:
    def __init__(self):
        self.signal_collectors = {
            "DeltaFilings": DeltaFilingsCollector(),
            "NoveltyVelocity": NoveltyVelocityCollector(),
            "TruthMeter": TruthMeterCollector(),
            "ConsensusGap": ConsensusGapCollector()
        }
        self.attribution_engine = AttributionEngine()
        self.document_store = DocumentStore()
        
    async def generate_pack(self, ticker, as_of=None):
        if as_of is None:
            as_of = datetime.utcnow()
        
        # Collect all signals
        signals = await self.collect_signals(ticker, as_of)
        
        # Calculate surprise index
        surprise_index = await self.calculate_surprise_index(signals)
        
        # Get regime probabilities
        regime_probs = await self.get_regime_probabilities(as_of)
        
        # Generate investment decision
        decision = await self.generate_decision(
            ticker, signals, surprise_index, regime_probs
        )
        
        # Collect all evidence
        evidence = await self.collect_evidence(ticker, signals)
        
        # Build evidence pack
        pack = {
            "as_of": as_of.isoformat(),
            "ticker": ticker,
            "surprise_index": surprise_index,
            "regime_probs": regime_probs,
            "signals": self.format_signals(signals, evidence),
            "consensus_gap": signals.get("ConsensusGap", {}).get("value", 0),
            "decision": decision,
            "metadata": self.generate_metadata(ticker, as_of)
        }
        
        # Store pack for audit
        await self.store_evidence_pack(pack)
        
        return pack
```

### Evidence Collection
```python
class EvidenceCollector:
    def __init__(self):
        self.sources = {
            "edgar": EDGARSource(),
            "news": NewsSource(),
            "transcripts": TranscriptSource(),
            "market": MarketDataSource()
        }
        
    async def collect_for_signal(self, signal_type, signal_data, ticker):
        evidence = []
        
        # Get relevant documents
        documents = await self.get_relevant_documents(
            signal_type, signal_data, ticker
        )
        
        # Score and rank documents
        scored_docs = await self.score_documents(documents, signal_data)
        
        # Select top evidence
        top_evidence = sorted(
            scored_docs, 
            key=lambda x: x["relevance_score"], 
            reverse=True
        )[:10]
        
        # Format evidence items
        for doc in top_evidence:
            evidence_item = {
                "source_type": doc["type"],
                "source_url": doc["url"],
                "timestamp": doc["timestamp"],
                "relevance_score": doc["relevance_score"],
                "snippet": self.extract_snippet(doc, signal_data),
                "section": doc.get("section"),
                "metadata": {
                    "source": doc["source"],
                    "language": doc.get("language", "en"),
                    "credibility": doc.get("credibility_score", 1.0)
                }
            }
            evidence.append(evidence_item)
        
        return evidence
    
    def extract_snippet(self, document, signal_data, max_length=500):
        # Find most relevant section
        relevant_text = self.find_relevant_section(
            document["content"], 
            signal_data["keywords"]
        )
        
        # Truncate if needed
        if len(relevant_text) > max_length:
            relevant_text = relevant_text[:max_length-3] + "..."
        
        return relevant_text
```

### Attribution Engine
```python
class AttributionEngine:
    def __init__(self):
        self.attribution_models = {
            "shapley": ShapleyAttributor(),
            "attention": AttentionAttributor(),
            "gradient": GradientAttributor()
        }
        
    def calculate_attribution(self, signal, documents):
        attributions = {}
        
        for model_name, model in self.attribution_models.items():
            attr_scores = model.attribute(signal, documents)
            attributions[model_name] = attr_scores
        
        # Ensemble attribution scores
        final_attribution = self.ensemble_attributions(attributions)
        
        return final_attribution
    
    def ensemble_attributions(self, attributions):
        # Average across different attribution methods
        ensemble = {}
        
        for doc_id in attributions["shapley"].keys():
            scores = [
                attributions[method].get(doc_id, 0)
                for method in attributions
            ]
            ensemble[doc_id] = np.mean(scores)
        
        # Normalize
        total = sum(ensemble.values())
        if total > 0:
            ensemble = {k: v/total for k, v in ensemble.items()}
        
        return ensemble
```

## Example Evidence Pack

### AAPL Evidence Pack (January 15, 2025)
```json
{
  "as_of": "2025-01-15T16:00:00Z",
  "ticker": "AAPL",
  "surprise_index": 0.73,
  "regime_probs": {
    "risk_on": 0.62,
    "risk_off": 0.18,
    "transition": 0.15,
    "goldilocks": 0.05
  },
  "signals": [
    {
      "type": "DeltaFilings",
      "value": 0.45,
      "confidence": 0.89,
      "evidence": [
        {
          "source_type": "filing",
          "source_url": "https://www.sec.gov/Archives/edgar/data/320193/000032019324000012/aapl-10q.htm",
          "timestamp": "2025-01-10T21:30:00Z",
          "relevance_score": 0.92,
          "snippet": "The Company faces increased regulatory scrutiny in the European Union regarding App Store practices, which may materially impact service revenues...",
          "section": "Risk Factors",
          "metadata": {
            "source": "SEC EDGAR",
            "filing_type": "10-Q",
            "accession": "0000320193-24-000012"
          }
        }
      ]
    },
    {
      "type": "NoveltyVelocity",
      "value": 0.81,
      "confidence": 0.76,
      "evidence": [
        {
          "source_type": "news",
          "source_url": "https://www.reuters.com/technology/apple-ai-partnership-2025-01-15",
          "timestamp": "2025-01-15T08:15:00Z",
          "relevance_score": 0.88,
          "snippet": "Apple announces exclusive partnership with leading AI research lab for next-generation Siri capabilities, marking significant strategic shift...",
          "metadata": {
            "source": "Reuters",
            "language": "en",
            "credibility": 0.95,
            "propagation_count": 47,
            "languages_reached": ["en", "zh", "ja", "de"]
          }
        }
      ]
    },
    {
      "type": "TruthMeter",
      "value": 0.72,
      "confidence": 0.83,
      "evidence": [
        {
          "source_type": "transcript",
          "source_url": "https://seekingalpha.com/article/aapl-q1-2025-earnings-call",
          "timestamp": "2025-01-14T16:30:00Z",
          "relevance_score": 0.79,
          "snippet": "CEO: 'We remain confident in our ability to navigate the regulatory landscape' [Prosody: elevated pitch variance, 3.2s pause before response]",
          "metadata": {
            "source": "Earnings Call",
            "speaker": "CEO",
            "prosody_anomaly": 0.28,
            "hedge_density": 0.15
          }
        }
      ]
    }
  ],
  "consensus_gap": 0.35,
  "decision": {
    "action": "overweight",
    "horizon_days": 90,
    "confidence": 0.68,
    "rationale": "High surprise index with positive novelty signal suggests market under-appreciating AI partnership impact. Regime remains supportive.",
    "risk_factors": [
      "Regulatory overhang in EU",
      "Elevated prosody anomalies in earnings call suggest management uncertainty"
    ],
    "exit_conditions": [
      "Surprise index drops below 0.4",
      "Regime shifts to risk-off",
      "Consensus gap closes below 0.1"
    ]
  },
  "metadata": {
    "generation_time_ms": 342,
    "data_completeness": 0.94,
    "model_versions": {
      "surprise_index": "v2.3.1",
      "regime_detection": "v1.8.0",
      "signal_generators": "v3.1.0"
    },
    "audit_id": "ep_20250115_aapl_7d3f8a92"
  }
}
```

## Evidence Quality Assurance

### Validation Pipeline
```python
class EvidenceValidator:
    def __init__(self):
        self.validators = {
            "url": URLValidator(),
            "timestamp": TimestampValidator(),
            "content": ContentValidator(),
            "attribution": AttributionValidator()
        }
        
    def validate_evidence_pack(self, pack):
        validation_results = {
            "is_valid": True,
            "errors": [],
            "warnings": [],
            "quality_score": 1.0
        }
        
        # Validate structure
        if not self.validate_schema(pack):
            validation_results["is_valid"] = False
            validation_results["errors"].append("Invalid schema")
            return validation_results
        
        # Validate each evidence item
        for signal in pack["signals"]:
            for evidence in signal.get("evidence", []):
                item_validation = self.validate_evidence_item(evidence)
                
                if not item_validation["is_valid"]:
                    validation_results["errors"].extend(item_validation["errors"])
                
                validation_results["warnings"].extend(item_validation["warnings"])
                validation_results["quality_score"] *= item_validation["quality_score"]
        
        # Check completeness
        completeness = self.check_completeness(pack)
        if completeness < 0.8:
            validation_results["warnings"].append(
                f"Evidence completeness below threshold: {completeness:.2f}"
            )
        
        validation_results["is_valid"] = len(validation_results["errors"]) == 0
        
        return validation_results
```

### Source Verification
```python
def verify_source(self, source_url, source_type):
    verification = {
        "is_verified": False,
        "verification_method": None,
        "confidence": 0.0
    }
    
    if source_type == "filing":
        # Verify against EDGAR
        if "sec.gov" in source_url:
            verification["is_verified"] = self.verify_edgar_url(source_url)
            verification["verification_method"] = "EDGAR_API"
            verification["confidence"] = 1.0 if verification["is_verified"] else 0.0
            
    elif source_type == "news":
        # Verify against known news sources
        domain = urlparse(source_url).netloc
        if domain in TRUSTED_NEWS_SOURCES:
            verification["is_verified"] = self.check_url_exists(source_url)
            verification["verification_method"] = "URL_CHECK"
            verification["confidence"] = 0.9 if verification["is_verified"] else 0.0
            
    elif source_type == "transcript":
        # Verify transcript sources
        verification = self.verify_transcript_source(source_url)
    
    return verification
```

## Storage and Retrieval

### Evidence Pack Storage
```python
class EvidencePackStore:
    def __init__(self):
        self.primary_store = PostgreSQL()
        self.document_store = S3()
        self.cache = Redis()
        
    async def store_pack(self, pack):
        # Generate unique ID
        pack_id = self.generate_pack_id(pack)
        pack["id"] = pack_id
        
        # Store metadata in database
        await self.primary_store.insert(
            "evidence_packs",
            {
                "id": pack_id,
                "ticker": pack["ticker"],
                "as_of": pack["as_of"],
                "surprise_index": pack["surprise_index"],
                "decision": json.dumps(pack["decision"]),
                "created_at": datetime.utcnow()
            }
        )
        
        # Store full pack in S3
        await self.document_store.put(
            f"evidence-packs/{pack_id}.json",
            json.dumps(pack),
            metadata={
                "ticker": pack["ticker"],
                "date": pack["as_of"],
                "compression": "gzip"
            }
        )
        
        # Cache for quick access
        await self.cache.set(
            f"pack:{pack['ticker']}:latest",
            json.dumps(pack),
            ex=3600  # 1 hour TTL
        )
        
        return pack_id
    
    async def retrieve_pack(self, pack_id=None, ticker=None, as_of=None):
        # Try cache first
        if ticker and not pack_id:
            cached = await self.cache.get(f"pack:{ticker}:latest")
            if cached:
                return json.loads(cached)
        
        # Query database
        if pack_id:
            query = {"id": pack_id}
        elif ticker and as_of:
            query = {"ticker": ticker, "as_of": as_of}
        else:
            raise ValueError("Must provide pack_id or ticker+as_of")
        
        metadata = await self.primary_store.find_one("evidence_packs", query)
        
        if not metadata:
            return None
        
        # Retrieve full pack from S3
        pack_data = await self.document_store.get(
            f"evidence-packs/{metadata['id']}.json"
        )
        
        return json.loads(pack_data)
```

## API Endpoints

### REST API
```python
@app.get("/api/v1/evidence/{ticker}")
async def get_evidence_pack(
    ticker: str,
    as_of: Optional[datetime] = None,
    include_documents: bool = False
):
    """Retrieve evidence pack for a ticker"""
    
    # Generate or retrieve pack
    if as_of:
        pack = await pack_store.retrieve_pack(ticker=ticker, as_of=as_of)
    else:
        pack = await pack_generator.generate_pack(ticker)
    
    if not pack:
        raise HTTPException(404, "Evidence pack not found")
    
    # Optionally include full documents
    if include_documents:
        pack["documents"] = await collect_full_documents(pack)
    
    return pack

@app.post("/api/v1/evidence/validate")
async def validate_evidence_pack(pack: dict):
    """Validate an evidence pack"""
    
    validator = EvidenceValidator()
    validation_result = validator.validate_evidence_pack(pack)
    
    return validation_result

@app.get("/api/v1/evidence/{ticker}/history")
async def get_evidence_history(
    ticker: str,
    start_date: datetime,
    end_date: datetime,
    limit: int = 100
):
    """Get historical evidence packs for a ticker"""
    
    packs = await pack_store.get_historical_packs(
        ticker, start_date, end_date, limit
    )
    
    return {
        "ticker": ticker,
        "count": len(packs),
        "packs": packs
    }
```

## Visualization Components

### Evidence Graph
```python
def generate_evidence_graph(pack):
    """Generate network graph of evidence relationships"""
    
    import networkx as nx
    
    G = nx.DiGraph()
    
    # Add central node
    G.add_node("decision", type="decision", **pack["decision"])
    
    # Add signal nodes
    for signal in pack["signals"]:
        signal_id = f"signal_{signal['type']}"
        G.add_node(signal_id, type="signal", **signal)
        G.add_edge(signal_id, "decision", weight=signal["value"])
        
        # Add evidence nodes
        for i, evidence in enumerate(signal.get("evidence", [])):
            evidence_id = f"evidence_{signal['type']}_{i}"
            G.add_node(evidence_id, type="evidence", **evidence)
            G.add_edge(evidence_id, signal_id, weight=evidence["relevance_score"])
    
    return G
```

## TODO Items
- `TODO(frontend, 2025-02-01)`: Build interactive evidence pack viewer
- `TODO(ml-team, 2025-02-04)`: Implement learned attribution models
- `TODO(data-team, 2025-02-07)`: Set up evidence pack archival system
- `TODO(product, 2025-02-10)`: Define evidence pack templates by user type